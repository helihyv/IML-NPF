\documentclass[a4size, 12pt]{report}
	\usepackage{amsmath} 
	\usepackage{amssymb}
\begin{document}
	
	\author{Group 19 
		
		– Heli Hyvättinen}
	
	\title{Introduction to Machine Learning
		
		 – Term project final report }
	
	\maketitle
	

	
	\section*{Introduction}
	
	The subject of this work is modelling the phenomenom of new particle formation (NPF) with machine learning methods. In new particle formation small particles in the air start to form larger particles. The classifiers created in this work model under what conditions NPF happens. The given task was to create and use a classifier for predicting the event types (including no event) for days in the given test dataset. The primary task was set to be predicting whether there was an event or not. In addition to the predicted event types, the propability of an NPF event happening and an estimation of accuracy of predicting corretly whether there was an event were reported.     
	
	At first an initial binary classifier that predicts just whether NPF happens was created. The second and final classifier is a multi-class classifier that is used to predict not just whether, but also what kind of NPF happens.  The latter classifier was the one used to create the separately reported prediction, propability and accuracy results.
 
		The exacts model used were selected with 10-fold cross validation.  Logistic regression and random trees were preliminarily selected as models to be included in the cross-validation. Both models were included in several versions with differing parameters. The model with the best mean cross-validation classification accuracy was selected. As some of the models used would not have provided the requested propabilities for classess I was prepared to favor a model that provides those probabilities if two or more models would be close to an equal performance. It turned out however, that the model with the best cross-validation accuracy provided the probabilities. Logistic regression with cost of 1 and $l_1$ penalty performed best for both the initial binary and teh final multiclass classification tasks.   

	
	\section*{The Data}
	
	The data used consist of daily means and standard deviations of measurements of several conditions and the occurrence of NPF evens at the Hyytiälä forestry field station between sunrise and sunset. 	 There are 50 differenet conditions represented in the data. They include e.g. temperatures at different heigths of the measuring mast, radiation (eg. UV-A) and amounts of substances in the air(eg. $No_x$, $CO_2$). The data wass guaranteed not to have missing values 
	
	 There are three kinds of NPF events in the data, labeled "Ia", "Ib" and "II". Labels "Ia" and "Ib" are used for the days on which growth and formation date was determined with a good confidence level (Maso et al. 2005). Other NPF events are labeled "II". Class "Ia" means that a clear and strong NPF event happened on that day, while the label "Ib" is used for other days that fulfill the conditions for label "I")  The fourth possibility for given day is that no NPF event happened at all. The data has been reported to have an equal amount of event ands nonevent days despite the nonevent days being more frequent in real life. 
	 

	
 
	
	\section*{Preprocessing data}
	
		For the separate binary classifier a new categorical variable "class2" was created that had the value "event" on the days any of the events occurred and "nonevent" otherwise. Existing variable "class4", that includes also the information of the occurrence of different events was defined to be a categorical variable. 
		
		Of the variables in the dataset given, index and date were excluded as irrelevant. The variable "partlybad" was excluded as useless since all the observations had the same value for it. After the removal of these variables the data had 100 features. There were 458 observations in the data. 
		 
		When used with Logistic Regression models the features were standardized to have a mean of zero and unit variance. For Random Forests no standardization was used.  

	
	\section*{Preliminary selection of models}
	
	Linear models usually give better results if the modelled relationships really are linear, while for non-linear relationships other models tend to be better (James et al. 2021). Because it was not known beforehand whether the relationships between the given features and NPF were linear or not, one linear and one non-linear model type was included in the cross-validation phase.  
	
	Logistic regression and linear discriminant analysis (LDA) are common linear models used in machine learning  (James et. 2021). LDA assumes features to be normally distributed. It also assumes that the features have a common in-class covariance matrix. Logistic regression does not make these assumptions. Considering the data has 100 features, it is not very likely that they would all share common covariance matrices. Therefore logistic regression is chosen as the linear model tried instead of LDA. Logistic regression is based on estimating the probabilities of the classes by fitting a linear model to the data  (James et. 2021). Because of this it does provide predicted probabilities for the classes.  
	  
	Random forests was chosen for the non-linear model. It is known to be an effective model and does not make assumptions on the features. 
	
	Random forests is based on generating many decision trees and combining them for predicting (James et al. 2021). While creating individual decision trees, only a random sample of features is considered at each split. As a downside, random forests does not provide any predictions of propabilities of classes. 
	
	Naive Bayes classification is based on the assumption that the features are independent of each other (James et al. 2021). Some of the features are clearly dependent of each  other, such as amounts of UV-A and UV-B radiation and temperatures at different heigths of the same measuring mast. Therefore the assumption of independence of features in the naieve Bayes classification is obviously not met. Because of this, Naive Bayes classification was not used. 
	
	For K-nearest neighbours (KNN) to provide accurate predictions, it requires the amount of observations to be much larger than the amount of features (James et al. 2021). The NPF data has 458 observations and 100 features. This suggests that there may not be enough observations in relation to the amount of features for KNN to provide accurate predictions. Because of this, KNN was not selected among the models tried.   

	The exact model used was selected with 10-fold cross validation. The model with the best mean cross-validation classification accuracy was selected. As some of the models used would not have provided the requested propabilities for classes I was prepared to favor a model that provides those probabilities if the models would be close to a equal performance. It turned out however, that the model with the best cross-validation accuracy provided the probabilities.  
		
	%-describe approaches considered
	%-approach chosen
	%-pros and cons of this approach
	
		\section*{Tools used}
	
	Python was used as the programming language. Numpy and pandas libraries were used for handling data. SciKit Learn (sklearn) was used as the machine learning library. 
	LogisticRegression from sklearn.linear\_models was used for Logistic regression. The StandardScaler function from sklearn.preprocessing was used for standardizing all features to have a mean of zero and unit variance for use with Logistic regression. RandomForestClassifier from sklearn.ensemble was used for Random Forests. The random seed was set to 555 troughout the program. 
	
	\section*{Feature selection}
	
   No prior feature selection was made. However some feature selection was done algorithmically as part of the classification process. 	
	
	Both classifiers used (LogisticRegression and RandomForestClassifier) give the possibility to perform (a sort of) feature selection. RandomForestClassifier limits the number of features included in a single tree by default. The default limit is $\sqrt{p}$, where $p$ is the number of features. As there are 100 features, maximum of 10 features is included in each tree. Models with this limit were included  in the cross-validation phase in both the initial and final models. For the initial model also the limit of $log_2 p$ and no limit at all were included. 
	
	 LogisticRegression uses $l_2$-penalty regularisation by default. It also allows using $l_1$-penalty regularisation.
	 
		When $l_1$ -penalty is used many of the coefficents become zero (Feature Selection 2021).
		
		 This reduces the amount of features that affect the predictions. Because of this Logistic Regression models with $l_1$-penalty were included in the cross-validation. The default $l_2$-penalty were also included.
	
		
	%	https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
	
	
	%	https://scikit-learn.org/stable/modules/feature_selection.html#l1-feature-selection
	
	
	\section*{The Initial Binary Classifier}
	
	\subsection*{Cross-validation}
	
	At first a model predicting only whether an event occuured or not was created The exact model used in this initial binary classifier was selected with 10-fold cross-validation. The given training data was split to training+validation and testing datasest. The training + validation set had 240 observations. This was divided to 10 buckets of 24 observations each. Each model tried was fit 10 times with 9 buckets as training data and 1 bucket of validation data, so that each bucket wwas used as validfation data once.  
	
	The models included in the 10-fold cross validation were Logistic regression and Random Forests. For Logistic regression the data was scaled by means and variances for faster convergence. Despite of this, the maximum amount of iterations had to be rised to 5000, since some models did not converge with some data even with 4000 iterations.  For Random Forests the data was used as such. 
	
		Random Forests models with the default maximum included features, $\sqrt{p}=10$ were included in variants with 100, 200 and 300 trees. Another maximum for included features used was $log_2 p$ and models with 100, 200 and 300 trees were trained with it. Also Random forests with no limit on the amount of features were trained with versions of 100 and 200 trees. The best performing Random Forests models had 85 \% mean cross-validation accuracy. They were the ones with a limit on amount of features and 200 trees.  
	
		For logistic regression 'saga' solver was used. Solver is the actual algorithm used for fitting. Saga was selected beacuse it supports both $l_1$ and $l_2$ regularisations and would have two different ways of managing multiclass fitting when later used in multiclass classifier.  Both l1 and l2 regularisation were included with varying cost parameters. The $l_1$ penalty was tried with costs of 1 (the default), 0.01, 0.5,  0.7, 0.9,1.3,  1.5,and 5. The $l_2$ penalty was tried with costs 1, 0.05,  0.1, 0.5, 0.9 and 10. 
		Of these, the $l_1$ penalty with costs of 1 and 0,9 performed best, with mean cross validation accuracy of 87,5 \%. For the $l_2$ penalty the large and small values of cost had best results, 0.05 and 10 gave 85 \% accuracy.		
  
	
\subsection*{Model accuracy and finalizing the initial model}
	
	Of the equally best performing models in the cross-validation, the one with default cost was selected. This was Logistic regression with $l_1$ regularisation and cost of 1. Such model was then trained with the entire training+validation data and predictions generated for the testing data that was put aside in the beginning. The test set had 218 observations. The accuracy of this classifier on the test data was $0.8486238532110092 \approx 84,9 \%$. 
	
	The final binary classification model was then trained with the entire data set (including training+validation and test sets). As training with more data rarely decreses accuracy, the accuracy of this final model can be estinated to be at least 84,9 \%.  
	
	As a result of the $l_1$ regularisation the amount of non zero-coefficients was reduced to only 49 of 100. This would be much from the point of view of interpretability, but here the goal is just predicting. 
	
		This model was however not used as a basis of the final model used for creating the final predictions. Instead another model, a four class classifier was created in a similar way and used.
	
	\section*{The final Multi-label Classifier}
	
	
	\subsection*{Cross-validation}
	
	The final model that was used in the predictions predicts also the type of the event. The exact model used for this four-class classification task was selected by 10-fold cross validation. The trainig + validation / test split and division of rows to buckets was kept the same as that used in the binary classification task. The given training data was split to training+validation and testing datasest. The training + validation set had 240 observations. This was divided to 10 buckets of 24 observations each. Each model tried was fit 10 times with 9 buckets as training data and 1 bucket as the validation data, so that each bucket was used as validation data once. 
	
	Logistic regression and random forest models were included in the 10-fold cross validation. Three random forest models with 100, 200 and 300 trees each were included. Because the other amount of included features had not given better results in the binary classifier, only the default amount of included features per tree was used. This was $\sqrt{p}$ maximum included features, which for this data is 10 features. 
	
	Logistic regression  with the saga solver, $l_1$ regularization and cost of one, which was best in the 2-class classification was included as two versions. First one used multinomial as the multiclass parameter. The second one used ovr as the multiclass parameter. In multinomial all classes are fitted simultaneously. OVS refers to One-vs-Rest, and in it each class is fitted agains all others in turn. 
	
The model with the saga solver, $l_1$ regularization and multinomial was included also with the costs $0.5$, $0.1$ and $10$. 	In addition Logistic regression with the saga solver, $l_2$ regularization, cost of one and multinomial multiclass was included
	
The best accuracy ($0.6833333333333333 \approx 68,3 \%$) was again reached with logistic regression, $l_1$ regularisation and cost of one. Multinomial performed better than ovr. The best random forest model was the one with 100 trees, with $\approx 65,4$ \% correcly classified.  
	
\subsection*{Model accuracy and finalization of the model}

	The best model had been logistic regression with the saga solver, $l_2$ regularization, cost of one and multinomial as multiclass parameter. A model identical to the best one was then trained with the entire training+validation data. Predictions for the test data were then generated with the model. for the testing data that was put aside in the beginning. The test set had 218 observations. The accuracy of this classifier on the test data was $0.6375000000000001 \approx 63,8 \% $ for the four-class classification task.	The binary classification accuracy (event/nonevent) was much better, $0.8532110091743119 \approx 85,3 \%$.  The binary classification accuracy of four-label classifier was on the same level and even sligthly better than the accuracy of the initial binary-only classifier on the test data.
	
	The final four-class classification model was then trained with the entire data set (including training+validation and test sets). As training with more data rarely decreses accuracy, the accuracy of this final model can be estimated to be at least 85,3 \%. Thus, it was rounded up to 0.86 when reporting the estimated binary accuracy when reported in the separate answers.csv file. %This turned out to be over-optimistic, as the actual result of 0.8538860 was very close to the accuracy achieved on the test set. 
	
	This final model was used to create the four-class predictions and binary-classification probabilites reported in the separate asnwers.csv file.
	
		As a result of the $l_1$ regularisation the amount of features with non zero-coefficients was reduced to only 26/100 for class II, 28/100 for the class Ia, 24/100 for the class Ib and 34/100 for the class "nonevent". Surprisingly there was quite a difference between the classes of what features were excluded. A total of 66 of 100 features had a non-zero co-efficient for at least one class. This would be way too much to make the model interpretable, but here the goal was just to make predictions. 
	
	With hindsigth, it would have been better to start straigth from the four-class classifier instead of starting with the binary-only classifier. That would have given more time to optimize the final classifier. Also, it was probably not a good idea to limit the parameters tried on the four-class classifier on basis of the results from the binary classifier, especially considering how different the models turned out to be in what features they included with non-zero co-efficients.
	
\section*{Summary}

The final model created predicts types (including no event) of daily NPF events. The 
model is given the daily means and variances of all the 50 measurements as data, but the 
model uses Logistic Regression with $l_2$ regularization, that has driven the 
coefficients of many of these features to zero. The cost parameter of the model is set 
to 1 and the model uses multinomial handling of multiple classes. The accuracy of the 
model in predicting the type of events can be expected to be at least $63,75$ \% 
correctly predicted. When the correctness of the predictions is inspected on the level 
of event/nonevet only, the accuracy is expected to be much better, at least 
approximately $0,853$  \% correctly predicted.


%The final model was 


%For some reason I had at some point misunderstood that separate binary and multiclass classifiers would be needed. With hindsigth, it would have been better to only create the multiclass classifier and try more combinations of different model parameters directly in it. 
	

	     

\section*{References}

James, G., Witten, D., Hastie, T. and Tibshiran, R. 2021: An Introduction to Statistical Learning with Applications in R. Second Edition. Springer. https://web.stanford.edu/~hastie/ISLR2/ISLRv2\_website.pdf  

Maso, M., Kulmala, M. Riipinen, I., Wagner, R., Hussein, T., Aalto, P.P. and Lehtinen, K.E.J. 2005: Formation and growth of fresh athmospheric aerosols: eight years of aerosol size distribution data from SMEAR II, Hyytiälä, Finland. Boreal Environment Research 10:323-336. 

Feature Selection 2021. https://scikit-learn.org/stable/modules/feature\_selection.html  Cited 11.12.2021. SciKit Learn User Guide. 
%	Self-grading report

\pagebreak
\section* {Self-grading report} 

I would give this work a grade of 3.

Most of the work expected in the assignment is done. The model used is in predictions, how it was selected and how it performed have been reported. However the description of the data is shallow and does not include use of any statistical methods of looking at the data. (None were used.)

I am not sure whether the feature selection used is sufficient to fullfill the expectation or whether prior selection of features would have been required. If not, I would drop the grade to 2. 

The reasoning behind the choise of models included in the cross-validation phase is stated and shows some understanding of the models considered. The description of the models used is given, but is very brief. Language is clear and rather scientific. The used terms have mostly been defined, but some of the definitions may be too superficial.      

\end{document}